
\begin{abstract}

% We use the Internet without know that the Internet is actually super reliable. It's based more upon: hey, the Internet seems reliable. 

% The Internet is used today for communication

% Detection of Internet outages, which are potentially rare events, demands broad and longitudinal measurements of users' Internet connections.
% Internet reliability is increasingly important as the applications we use increasingly depend upon the Internet.
% Internet reliability is increasingly important as a variety of services that we use migrate to the Internet.
% Internet reliability is increasingly important as the applications that we depend upon migrate to the Internet.
Internet reliability is increasingly important as a variety of services that we use migrate to the Internet. Yet, we lack authoritative measures of last-mile Internet reliability. The first step towards measuring last-mile reliability is to detect Internet outage events experienced by users. Since Internet outages are rare events, detecting them requires broad and longitudinal measurements; however, such measurements of Internet reliability at the individual user level are challenging to obtain accurately and at scale. The second step is to use detected outages to reason about Internet reliability across different dimensions such as ISPs, media-types, and geographical areas.

Probing-based remote outage detection techniques can scale but their accuracy is questionable. These techniques detect Internet outages across time as well as across the IPv4 address space by sending active probes, such as pings and traceroutes, to users' IP addresses and use probe responses to infer Internet connectivity. However, they can infer false outages since their foundational assumption can sometimes be invalid: that the lack of response to an active probe is indicative of failure. I illustrate two potential scenarios where this assumption is invalid. In the first scenario, responses are delayed beyond the prober's timeout, leading these techniques to infer packet-loss instead of delay. In the second scenario, these techniques can falsely infer packet-loss when the address they are probing gets dynamically reassigned. I examine how commonly delayed responses and dynamic reassignment occur across ISPs to quantify the inaccuracy of these techniques. 

Next, I demonstrate how detected outages can be used to perform meaningful assessments of Internet reliability. One aspect of Internet reliability is the study of how an external factor (like the occurence of thunderstorms) affects Internet connectivity; I show how to study the effect of such a factor upon the reliability of a group of addresses by studying the \emph{inflation} in outage rate for that group during its presence. Measuring the inflation in outage rate mitigates the effects of false outages. I also develop and evaluate an approach to segregate outages into categories that suggest their cause. Outages could result from a variety of causes, such as power outages, voluntary shutdown of users' home Internet equipment, network outages due to an ISP's infrastructure failure etc. When assessing the reliability of a particular ISP, we would ideally consider only the subset of outages that affect solely that ISP. I propose a technique to segregate outages by detecting simultaneous outages of ``related'' groups of addresses (addresses may be related by geography, ISP, or network topology). Simultaneous outages can serve as evidence that a detected outage affected multiple users in a particular ISP.

Implementing these proposed techniques will help achieve comprehensive measurements of Internet reliability that can be used to identify vulnerable networks and their challenges, inform which enhancements can help networks improve reliability, and evaluate the efficacy of deployed enhancements over time.

% comprehensive datasets of Internet reliability that can aid in improving reliability by targeting problem regions or adapting strategies that have proven to be successful
\end{abstract}
