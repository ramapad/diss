
% \section{Preliminaries}

\chapter{Introduction}


% With online accessibility of
% devices ranging from temperature control systems to baby monitors in
% this Internet of Things era, our dependence upon the Internet for
% increasingly many facets of our daily lives only continues to
% rise.

% With the ability to access a
% variety of devices online in this Internet of Things era, ranging from
% temperature control systems to baby monitors, our dependence upon the
% Internet for increasingly many facets of our daily lives only
% continues to rise. 

% As the range of Internet services that we rely upon increases, so does our reliance upon
% the Internet. 

% Talk more about how important the Internet is?

% Perhaps find a couple of recent publications that state how common IoT is becoming.

Residential Internet reliability is increasingly important as a variety of
services that we use migrate to the Internet. Internet users today can
communicate with each other, perform financial transactions, plan
their travel, and even obtain critical services such as health
monitoring~\cite{ideal-life, remote-health-elderly} and emergency
services~\cite{emergency-voip-voipfone, emergency-voip-fcc} from their
homes. Our dependence upon the Internet is rising further as more of
our home devices become connected. Consequently, reliable residential Internet connectivity is critical.

Broad and longitudinal measurements of users' Internet reliability can
identify vulnerable networks, these networks' challenges, and
potential enhancements. For instance, weather conditions such as
thunderstorms, rain, and gales, can adversely affect Internet
reliability~\cite{pingin}. Measurements can inform which areas are
particularly vulnerable to weather conditions. Comparing measurements
against other areas with similar weather conditions can provide
insights on potential enhancements: for example, areas may be less
vulnerable to gales where Internet cables run underground. Once an
enhancement is deployed, measurements can reveal if the enhancement
has resulted in improved Internet reliability.

% Talk about how Internet reliability is difficult and who is
% interested. FCC. Even ISPs (Cite the "Nevermind" paper by Nick
% Duffield which claims that ISPs are typically reactive and wait for
% customers to call). Common users.
 
The inferences from residential Internet reliability measurements can
benefit various stakeholders, including policymakers,
Internet Service Providers (ISPs), and residential users
themselves. Policymakers around the world have begun efforts to
measure Internet reliability~\cite{measuring-broadband-america,
measuring-broadband-canda, ofcom-uk-broadband-research,
measuring-broadband-australia}, since such measurements can drive
incentives and policies to improve reliability. ISPs can benefit from
these measurements in multiple ways. Since even large ISPs rely upon
their users to inform them of network connectivity
issues~\cite{conext10-jin}, they may be unaware of problems in their
own networks; these measurements can help ISPs recognize underyling
problems. Further, ISPs can learn about the reliability of their
competitors. Measurements of Internet reliability will also benefit
residential users, since they can take into account the reliability of
ISPs in their geographic region when purchasing Internet services.

However, we currently lack authoritative measures of residential
Internet reliability, due to several challenges associated with
obtaining such measures. 

% Measuring
% Internet reliability at the individual user level will therefore prove invaluable in assessing
% the current state of Internet connectivity and in developing future
% enhancements.

% By detecting outage events and their duration, we
% can reason about a particular user's Internet reliability and compare
% it to other users' reliability across geography, ISPs, and
% media-types.

% \section{Background, challenges, state of the art}
% \section{Background}
\section{Background: state of the art in measuring residential Internet reliability}

% The problem is: how much do I cover in background? Using outages to
% understand reliability is not something I can cover too much. I can
% talk at length about outage detection: about false positives, about
% going after individual outages etc. But very little about the actual metrics.

Intuitively, a reliable Internet connection is one that works
continuously. In other words, it experiences no outages. 

% Measuring residential Internet reliability requires the detection of
% \emph{Internet outages} affecting residential customers---at a high
% level, these are events that prevent users from communicating over the
% Internet.

Measuring Internet reliability, therefore, necessitates measuring
\emph{Internet outages} and then using measured outages in a metric that
represents some property of outages. At a high level, an Internet
outage is an event that prevents users from communicating over the
Internet. Since we expect outage events to be rare, detecting them
requires monitoring a broad set of residential users over long
periods. After detecting outages over time for a group of residential
addresses---say addresses belonging to the same ISP or geographic
region---the detected outages can be used to calculate reliability
metrics. An example metric is the \emph{outage rate}, the number of
outages occurring over time for a group of addresses. These metrics
can be used to compare different groups of addresses and to identify
groups with particularly low reliability.

\subsection{Challenges}

% TODO: Should I expand on "broad"? They need to be "broad" because
% reliability could be different in different places and at different
% times and in different networks. 

% TODO: Should I try convincing people that individual user
% measurement is necessary? Why not just do Trinocular?

Detecting residential Internet outages is challenging. The first challenge is the scale of residential users on the Internet: there
are millions of residential Internet connections to monitor. Further,
residential Internet outages can vary in the number of
affected users. They can affect entire cities during large power
outages. They can also affect just an individual house if a fallen
tree branch damages the last-mile link between the house and the
ISP.  Another challenge is the heterogeneity of residential Internet
connections. Some connections are cable connections, where the home router
typically has a stable public IPv4 address. Others are DSL
connections where the address assigned to the home router can change
every 24 hours. Residential Internet connections can also use
satellite links, which are prone to higher latencies. 

Designing an outage detection system that can measure users
broadly and yet remain accurate across diverse heterogeneous
Internet connections remains a challenge.

\subsection{Prior approaches}

Prior approaches tradeoff either outage detection breadth or
accuracy. 

Edge network outage detection techniques detect outages broadly but
focus upon detecting outages that affect a substanial numer of addresses
in a group collectively. The group may comprise addresses belonging to the same /24 address block~\cite{trinocular, advancing-outage-art}, BGP
prefix~\cite{hubble}, or country~\cite{dainotti-imc11}. Techniques seek such disruption events because individually, each large disruption has
impact and their size makes them easier to confirm, e.g., with operators. However, residential Internet
outages may be limited to a small neighborhood or apartment block; these
techniques are likely to miss such events. Thus, they trade off outage detection accuracy
for breadth.

% Broadly, current techniques can be grouped into on-premises
% and remote probing-based, with the former trading off breadth, and the
% latter, accuracy.
On-premises techniques, such as RIPE Atlas~\cite{atlas},
SamKnows~\cite{samknows}, BISmark~\cite{bismark-main-bib}, and
DIMES~\cite{netdimes} measure diverse aspects of users' Internet
connections, including connectivity problems, but measure relatively
few users. These techniques deploy dedicated hardware or software at
user premises that continuously conduct ping, traceroute, DNS
measurements etc.; some of these measurements can be used to infer
Internet connectivity problems.

% Another class of techniques trades off outage detection accuracy for
% breadth by detecting outages that affect entire network prefixes or address
% blocks. Hubble studies reachability problems affecting BGP
% prefixes~\cite{hubble}. Trinocular detects outages affecting /24
% address blocks. Richter et~al.~\cite{advancing-outage-art} use the
% observation point of a large CDN to detect periods of reduced activity
% from /24 address blocks consistent with outages. Dainotti
% et~al.~\cite{dainotti-imc11} observe Internet Background Radiation
% traffic sent to IPv4 darknets to detect outages affecting entire
% countries. These techniques are designed to detect large Internet
% outages broadly, but at the cost of missing smaller outages that do
% not affect the entire monitored aggregate.

Whereas on-premises techniques have fundamental scaling
difficulties owing to manufacturing and deployment costs, hundreds of
millions of IP addresses respond to active
probes~\cite{timeouts}. Since many residences have at least one device
with a public IP address ~\cite{cgn-imc16} (typically the home
router), these IP addresses can be probed remotely, from vantage
points under researcher control, to measure their
connectivity. Thunderping~\cite{pingin} and
Trinocular~\cite{trinocular} adopt this approach to outage detection:
they focus upon measuring only connectivity but do so for many
users. Since these techniques can send probes remotely from servers
under their control, without requiring any user involvement, they are
able to detect outages across time as well as across the IPv4 address
space.

% The Thunderping technique monitors individual users' residential
% Internet connections during times of severe weather by sending active
% probes to residential addresses and analyzing responses. Hundreds of millions of IP
% addresses respond to active probes~\cite{timeouts}, allowing . Since many
% residences have at least one device with a public IP address
% ~\cite{cgn-imc16} (typically the home router), these IP addresses can
% be probed remotely, from vantage points under researcher control, to
% measure their connectivity. Thunderping~\cite{pingin} adopts this
% approach to outage detection: it focus upon measuring only
% connectivity but does so for millions of residential Internet
% connections. Since Thunderping can send probes remotely from servers
% under its control, without requiring any user involvement, it is able
% to detect outages across time as well as across the IPv4 address
% space.

%TODO: Tie back to heterogeity, if possible

Though capable of measuring Internet outages broadly, probing-based
remote outage detection techniques can make false inferences about
outages when some scenarios occur~\cite{timeouts,
addrchange-reasons}. The likelihood of occurrence of these scenarios
varies across geographic regions, ISPs, and media type. Analyzing
outages in the presence of these confounding factors requires broad
measurements of these factors in turn.



% The first step towards measuring residential Internet
% reliability is to detect \emph{Internet outages}---events that prevent
% users from communicating over the Internet. Since we expect outage
% events to be rare, detecting them requires broad and longitudinal
% measurements. Further, residential Internet outages can vary in terms of the
% number of affected users. They can affect entire counties during large power
% outages. They can also affect just an individual house if a fallen tree
% branch damages the last-mile link. The ideal outage measurement system
% would capture all Internet outage events, including individual users'
% outages, in all kinds of networks.

% However, measuring outages at the individual user level is
% challenging. The scale of residential users in the Internet makes it
% difficult to measure outages broadly and the heterogeneity of
% residential networks presents problems in interpreting measurements
% and detecting outages accurately.

% Existing techniques that measure individual-user-level outages
% tradeoff either outage measurement breadth or accuracy. Broadly,
% current techniques can be
% grouped into on-premises and remote probing-based, with the former
% trading off breadth, and the latter, accuracy. On-premises techniques, such as
% RIPE Atlas~\cite{atlas}, SamKnows~\cite{samknows}, and
% BISmark~\cite{bismark-main-bib}, measure diverse aspects of
% users' Internet connections, but
% measure relatively few users. These techniques 
% deploy dedicated hardware at user premises that continuously conduct ping,
% traceroute, DNS measurements etc.; some of these
% measurements can be used to infer Internet connectivity problems. Whereas hardware
% based techniques have fundamental scaling difficulties owing to
% manufacturing and deployment costs, hundreds of millions of
% IP addresses respond to active probes~\cite{timeouts}. Since many
% residences have at least one device with a public IP address ~\cite{cgn-imc16}
% (typically the home router), these IP addresses can be probed
% remotely, from 
% vantage points under researcher control, to measure their connectivity. Thunderping~\cite{pingin} and Trinocular~\cite{trinocular} adopt this
% approach to outage detection: they focus upon
% measuring only connectivity but do so for many users. Since these
% techniques can send probes remotely from servers under their control, without requiring any user
% involvement, they are able to detect outages across time
% as well as across the IPv4 address space.




% Though capable of measuring Internet outages broadly, probing-based
% remote outage detection techniques can make false inferences about
% outages when some scenarios occur~\cite{timeouts,
% addrchange-reasons}. The likelihood of occurrence of these scenarios
% varies across geographic regions, ISPs, and media type. Analyzing
% outages in the presence of these confounding factors requires broad
% measurements of these factors in turn.
% Due to the heterogeneity of
% residential networks, these scenarios are particularly likely to occur
% in some networks, leading to 



% \subsection{Analyzing reliability using measured outages}

% TODO: Come back here and ensure this is consistent with the
% description in the Intro of chapter 2. And that the text in Chater 2
% is consistent with this.

% The next step towards analzying reliability is to use measured outages
% to make inferences about residential Internet reliability. Inferences
% can be made by employing metrics that capture outage properties. One such
% metric is the rate of Internet outages over time. Another
% metric is the fraction of total time accounted for by outages. 

% These metrics can quantify Internet reliability along different
% dimensions---such as reliability across ISPs, media-types, geographic
% regions---when facing a range of conditions. Comparing Internet
% reliability metrics across ISPs can help users with their choice of ISP and
% can help ISPs assess their competitors. Comparing metrics across geographic
% regions can help policymakers and ISPs identify problematic areas that
% can benefit from more Internet infrastructure investment. Comparing
% metrics for a particular group of addresses across conditions (such as
% the presence of rain) can reveal the effect of the condition on
% Internet reliability.

% However, not all Internet outages contribute towards
% (un)reliability metrics. For example, if a user voluntarily chooses to power
% off their home Internet equipment, the user has an Internet outage but
% this outage should not lower the user's ISP's Internet reliability. Similarly, a power
% outage in an area should contribute towards lowering the reliability
% of that area, but should not lower the reliability of the
% ISPs whose addresses were affected. 

% The ideal Internet reliability assessment system should be capable of
% categorizing detected outages by their cause. These categorized
% outages can then be used to determine Internet reliability across
% different dimensions.

% However, analyzing Internet reliability using measured outages is
% again challenging. First, outage measurement errors can result in
% overestimating Internet reliability problems. Second, categorizing
% outages by their cause is difficult. Prior work performs
% outage classification mostly using sparsely available ground
% truth. Turner et al. investigate router and system logs and email
% archives to find different kinds of infrastructure
% outages~\cite{california-fault-lines}. Banerjee et al. analyze the
% outages mailing list~\cite{outages-mailing-list} to study and
% categorize widespread
% outages~\cite{phillipa-outages-mailing-list}. However, these
% techniques rely upon incomplete datasets, since ground truth tends to
% exist only for large Internet infrastructure outages. Further, the
% ground truth datasets they study are biased: Turner et al. study logs only
% from the CENIC network in California and the outages mailing list has
% a North American bias. 

% Existing techniques that study invididual user
% outages have typically focused upon outage detection and have not
% attempted to categorize detected outages by their likely cause. Grover
% et al. point out that users in some countries are particularly likely
% to power down their home Internet equipment but do not attempt further
% classification of detected outages~\cite{grover2013peeking}. Shah et
% al.~\cite{disco} and Bischof et al.~\cite{alwayson} use traceroutes sent from within the
% home to find the hop where probes fail. While the traceroute-based
% approach has the potential to categorize outages---by correlating
% failures that fail at the same hop, for instance---neither technique currently
% attempts to infer the real world event (like a network or power
% outage) that resulted in the outage. However, analyzing the reliability of an
% ISP or the reliability of a geographic area requires the
% identification of outage causes, so that the appropriate subset of
% them can be used in Internet reliability metrics. 



% segregate detected outages into categories
% that suggest their cause. Once outages are categorized in this
% manner, we can estimate the reliability of an ISP by considering the
% subset of outages that affected solely that ISP.

% However, measuring residential
% outages is challenging because of the scale: there are millions of
% residential links to measure. 
% Second, users can voluntarily power
% down their home Internet equipment and it is challenging to
% distinguish between voluntary user shutdowns and an outage at the last-mile link.
 
% ; even multihomed last mile links for business connectivity
% often share the same upstream hardware, representing a single point of 
% failure~\cite{twcable-business-web}. 
% Last-mile links lack the redundancy of the Internet's core;
% thus an outage of the last-mile link is likely to cut off users'
% Internet connectivity altogether.

% The second challenge is that it can be hard to distinguish
% between an outage at the last-mile link and voluntary user shutdowns
% of their Internet connections.
% We do not have a good understanding of the reliabity of Internet
% connectivity for end-users. Understanding the reliability of Internet
% connectivity for end-users requires understanding the reliability of
% the \emph{last mile link} connecting an end-user to the Internet. This
% is because the core of the Internet is designed to be redundant but
% last mile links typically are not.
%TODO: Perhaps borrow sentence from enduser.tex about business last-mile links.


% Thesis statement comments

% Older versions of thesis statement:
% \emph{For any end-host with a publicly assigned IP address that has the ability to respond to active probes, it is possible to remotely isolate accurately determine connectivity problems experienced by that end-host's last mile link.}
% \emph{For any end-host with an IP address that has the ability to
% respond to active probes, it is possible to remotely detect outages
% experienced by that end-host's last mile link.}

% Bobby's comments: Send traceroutes, but also to related addresses. What happens when all addresses change en-masse? Can we somehow identify such instances?
% Neil suggested that I should replace end-host with 'Internet accessible device'. Can I assert that a device with a public IP address is by definition Internet accessible.
% \emph{It is possible to remotely and accurately detect outages experienced by any device with a public IP address that typically responds to active probes.}

% Come up with definition of accuracy of outages.

% and use it to compare reliability across ISPs, media-types and
% geographical areas

\section{Thesis}


% My goal in this dissertation is to use probing-based techniques to
% detect outages remotely and use detected outages for
% measuring Internet reliability.
% My goal in this dissertation is to develop approaches that will
% address the challenges in obtaining authoritative measures of
% residential Internet reliability. 

The goal of this dissertation is to provide broad, longitudinal, and accurate measurements of
Internet reliability across ISPs, media-types, and geographic
locations in a variety of circumstances. I work towards this goal using the
probing-based technique due to its ability to scale. In the rest of
the dissertation, I illustrate my approaches to mitigate probing-based
techniques' problems in measuring residential Internet reliability
by defending the following thesis:

\emph{It is possible to remotely and accurately detect substantial outages
  experienced by any device with a stable public IP address that typically
  responds to active probes and use these outages to compare
  reliability across ISPs, media-types, geographical areas, and
  weather conditions.} 

\begin{itemize}

\item {\emph{Device with a stable public IP address}: This is a device
    connected to the Internet, like a
home-router, to which an ISP has assigned a public IP address such
that the
assignment is either static, or dynamic in a manner that allows the
duration of dynamic assignment to be estimated.}

\item {\emph{Substantial outage}: I define a substantial outage to be an event where a device
    with an Internet connection is unable to send or receive any
    packets for at least 11 minutes. Such outages are likely to inconvenience residential users.}

\item {\emph{Accuracy of outage detection}: An outage detection technique is accurate when it
correctly identifies every substantial outage event experienced by an Internet-connected-device. There are no time-periods when the address
experiences a substantial outage but it goes undetected (false
negatives). Similarly, there are no time-periods classified as
outages when the destination address is able to receive packets from the
Internet (false positives).}

\item {\emph{Reliability}: I measure reliability using the outage
    rate metric, which I define as the raw count of outage
events over measured time.}

\end{itemize}

% The common theme is that it is possible to list, measure, mitigate
% My approaches share a common theme: I list potential sources of
% error, measure them, and mitigate them



% My approach to accuracy 

% My approach to accuracy is enumerate sources of error and evaluate
% whether they are creating an error. A good bit is focused on the
% experiments looking for and eliminating sources of error. As opposed
% to getting ground truth from some source and just evaluating against
% that.

% At the heart of my approaches is the insight that it is possible to: (a) list
% potential sources of error, (b) measure and evaluate these sources of
% error in various circumstances, and (c) use these
% measurements to mitigate error. I use this insight to list and
% measure the likelihood of potential errors in outage detection due
% to confounding factors (such as high latency and dynamic address reassignment) across ISPs and
% geographic regions. I also use this insight to list potential causes
% of unrelated outages (such as users powering off their
% home Internet equipment) and develop techniques to 
% measure and mitigate their effect on Internet reliability metrics.

 %  extracting instances where the signal
% clearly outweighs the noise.

% Negate them is true in the weather-paper. Not strictly true in
% corrfails. Think about how to do this. Ahh, just use mitigate again, whatever.

\section{Contributions}

To demonstrate the thesis, I measure two confounding factors---high
latency (Chapter~\ref{cpt:timeouts}) and dynamic address reassignent
(Chapter~\ref{cpt:addr_change})---that can lead probing-based outage
detection techniques to make false outage inferences. In
Chapter~\ref{cpt:corrfails}, I motivate the detection of individual
addresses' outages. I go on to show how
to measure Internet reliability in the presence of inference errors
and unrelated outages in Chapter~\ref{cpt:weather}. This dissertation is organized as follows:
 % I
% demonstrate that the inflation in outage rate during the presence of a
% particular challenging condition, such as during thunderstorms, 

% I make the following
% contributions:

% TODO: 
% It is possible to remotely and accurately estimate the reliability of
% an ISP's customer's device so long as the device has
% a stable public IP address that typically
%   responds to active probes

\textbf{Chapter~\ref{cpt:bg}:~~State of the art in residential Internet outage detection}

I provide background and place existing work in Internet outage
detection in context. I describe the challenges that probing-based
remote outage detection techniques will need to address to measure
residential Internet reliability. These techniques study outages by
sending active probes (such as ping's echo requests) and use probe
responses to infer outages. They assume that a response to an active
probe indicates a working path to the probed user device and that lack
of response is indicative of failure. I illustrate two scenarios where
this assumption can be invalid, leading to potentially false outage
inferences. 
 % I
% illustrate two potential scenarios where this assumption is
% invalid---when responses are delayed beyond the prober's timeout and
% when the probed address is dynamically reassigned. I also highlight why
% Internet outages voluntarily caused by the
% residential user need to be segregated.
% I also describe the challenge that remote probing-based techniques face
% in detecting outages specifically in the last-mile.

\textbf{Chapter~\ref{cpt:timeouts}:~~Mitigating false inferences due
to early timeout} 

I investigate the
prevalence of delayed probe responses due to early timeout. The lack
of response to an active probe isn't always indicative of loss; for
example, when responses are delayed beyond the prober's timeout, the
response eventually arrives but the prober would never see the
response because it timed out too early. I report how commonly
responses are delayed beyond timeouts in networks around the world and
use these measurements to discuss techniques that would mitigate this problem.
% Decouple probe retransmission
% and loss. Possibly identify that only cellular guys have long delays?
% Also, this will ensure that we trigger retransmission upon high
% latency/loss and actually identify loss as loss and high latency as
% high latency.

\textbf{Chapter~\ref{cpt:addr_change}:~~Mitigating false inferences due to dynamic addressing} 

I investigate how dynamic addressing can lead remote probing-based
outage detection techniques to make false inferences about outages and
techniques to mitigate these false inferences. I measure the frequency
and patterns in dynamic address reassignment for ISPs across the
world.  I also introduce a technique using a complementary dataset to
determine whether an outage detected for an address by a probing-based
system is a false outage due to dynamic reassignment. % The evidence for
% reassignment over failure can be as simple as a client that once asked
% for updates from the address with a detected outage asking for the
% next update \emph{during} the detected outage from a different address.

\textbf{Chapter~\ref{cpt:corrfails}:~~The need for measuring individual address outages} 

I motivate the need to study individual address outages by showing
that individual address outage measurements can be used to find outage
events that are statistically unlikely to have occurred independently, and that many of these events
would not be detected by prior work. I describe how to use
simultaneous outages of individual addresses related to each other, by
geography and ISP, to identify outages that are highly unlikely to have
occurred independently, and are therefore likely to share a common
underlying cause.

\textbf{Chapter~\ref{cpt:weather}:~~Analyzing weather's effect on
  Internet Reliability} 

I show how to measure and compare the reliability of groups of addresses---like
addresses belonging to the same ISP, media-type, geographic
region---when facing challenging environments. I consider one class of
challenging environments that residential Internet connections can
face: severe weather conditions. I show how to use the inflation in
outage rate to measure the effect
of different classes of weather upon various groups of addresses.

\textbf{Chapter~\ref{cpt:future}:~~Future Work} 

I describe directions for future work in measuring residential
reliability using probing-based techniques.

% We show in this proposal that confounding factors can cause RODWAP
% techniques to sometimes
% infer false outages. 

% However, we argue that RODWAP techniques can
% be used for accurate outage detection by identifying and mitigating
% confounding factors. 
 % This has
% been the basis of existing active probe based techniques that detect
% loss, and thereby outage events, such as Thunderping~\cite{pingin} and
% Trinocular~\cite{quan2013trinocular}.

% In spite of , we argue that it is possible to remotely detect
% outages on the last mile link using active probes for any end-host
% with an IP address that responds to active probes.

% We investigate potential causes that
% would lead existing active probe based outage detection approaches to
% falsely infer loss and describe proposed work to mitigate detection of
% false loss in Section~\ref{sec:timeouts} and
% Section~\ref{sec:addr_change}. We also propose 

% We show in this proposal that probe-loss need not always be due to
% lossy links.


% \begin{itemize}
%   \item{\bf{False probe-loss inference due to early timeout:}} 
%     Traditionally, active probe based approaches time out probes after a few seconds. Responses that arrive after the timeout will be reported as lost. When this happens, existing techniques would confuse high delay with probe-loss.
%   \item{\bf{False probe-loss inference due to IP address change:}}
%     Consider an IP address that was previously responsive. If the host to whom that IP address was assigned changed its IP address as a result of dynamic addressing or mobility, and if the probed IP address is not reassigned to any host, then echo responses will cease to arrive and existing techniques would infer false probe-loss.
% \end{itemize}

% After analyzing causes of false probe-loss, we will investigate techniques to remove false probe-loss. Once we remove false probe-loss, we will be left with all instances of true probe-loss and probe-delays. This will give us the ability to identify that \emph{some} link on the path to the destination is experiencing connectivity problems. However, since we are specifically interested in identifying connectivity problems on the last-mile link we require another step. In this step, we will conduct TTL-limited probing from multiple vantage points to identify which link exhibits connectivity problems.

